import logging
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
)
import pandas as pd
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import SentenceTransformerEmbeddings 
import rag_pipeline

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.getLogger("ragas").setLevel(logging.WARNING)


def prepare_evaluation_data(eval_questions: list) -> pd.DataFrame:
    """
    Prepares data in the format required by RAGAS by running queries through our RAG pipeline.
    """
    logging.info("Preparing evaluation data for RAGAS...")
    
    # Ensure RAG pipeline is initialized (loads models, index etc.)
    logging.info("Ensuring RAG pipeline is initialized...")
    rag_pipeline.initialize_rag_pipeline(force_rebuild_index=False) # Call initialization

    # Check if initialization was successful by accessing module's globals
    if not all([rag_pipeline._model, rag_pipeline._faiss_index, rag_pipeline._quote_data_list]):
        logging.error("Failed to initialize RAG pipeline for evaluation (components are still None).")
        return pd.DataFrame()

    questions = []
    answers = []
    contexts_list = []

    for q_idx, question_text in enumerate(eval_questions):
        logging.info(f"Processing evaluation question {q_idx+1}/{len(eval_questions)}: {question_text}")
        
        # Contexts from retrieved documents 
        # We need the raw retrieved documents (quotes text) for RAGAS context metrics
        # Access globals via the module name
        retrieved_docs_details = rag_pipeline.retrieve_quotes(
            question_text, 
            rag_pipeline._model, 
            rag_pipeline._faiss_index, 
            rag_pipeline._quote_data_list, 
            top_k=3
        )
        
        current_contexts = [doc['text'] for doc in retrieved_docs_details]
        if not current_contexts:
            logging.warning(f"No contexts retrieved for question: {question_text}")
    
        # 2. Get answer generated by LLM 
        # The query_rag function from rag_pipeline.py already does retrieval + generation
        # We need the 'summary' part as the 'answer' for RAGAS.
        rag_response = rag_pipeline.query_rag(question_text, top_k_retrieval=3) 
        
        generated_answer = rag_response.get('summary', "Error: Could not generate summary.")
        if "error" in rag_response:
             generated_answer = f"Error in RAG response: {rag_response.get('details', 'Unknown error')}"

        questions.append(question_text)
        answers.append(generated_answer)
        contexts_list.append(current_contexts)
        

    eval_data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts_list,
        
    }
    
    df = pd.DataFrame(eval_data)
    logging.info(f"Prepared evaluation data with {len(df)} entries.")
    return df

def run_ragas_evaluation(eval_df: pd.DataFrame):
    """Runs RAGAS evaluation on the prepared data."""
    if eval_df.empty:
        logging.error("Evaluation dataframe is empty. Cannot run RAGAS.")
        return None

    logging.info("Converting DataFrame to Hugging Face Dataset for RAGAS...")
    eval_dataset_hf = Dataset.from_pandas(eval_df)

    
    google_api_key_for_ragas = os.getenv("GOOGLE_API_KEY")
    ragas_llm_model_name = 'gemini-2.0-flash-001' 

    if not google_api_key_for_ragas:
        logging.error("GOOGLE_API_KEY not found. Cannot configure LLM for RAGAS metrics.")
        return None

    metrics_to_evaluate = [
        faithfulness,
        answer_relevancy,
    ]

    google_api_key_for_ragas = os.getenv("GOOGLE_API_KEY")
    ragas_llm_model_name = 'gemini-2.0-flash-001' 

    if not google_api_key_for_ragas:
        logging.error("GOOGLE_API_KEY not found. Cannot configure LLM for RAGAS metrics.")
        return None

    try:
        # Initializing the Langchain LLM wrapper for Gemini
        langchain_gemini_llm = ChatGoogleGenerativeAI(
            model=ragas_llm_model_name,
            google_api_key=google_api_key_for_ragas,
            temperature=0.0
        )
        logging.info(f"Configured Langchain Gemini LLM for RAGAS: {ragas_llm_model_name}")

        custom_embeddings_model_path = rag_pipeline.FINE_TUNED_MODEL_PATH
        
        if not os.path.exists(custom_embeddings_model_path):
            logging.error(f"Fine-tuned model for embeddings not found at {custom_embeddings_model_path}. Cannot run RAGAS with custom embeddings.")
            return None

        custom_embeddings = SentenceTransformerEmbeddings(
            model_name=custom_embeddings_model_path
        )
        logging.info(f"Configured SentenceTransformerEmbeddings for RAGAS using model: {custom_embeddings_model_path}")

        logging.info("Running RAGAS evaluation... This may take some time and make LLM calls.")
        result = evaluate(
            dataset=eval_dataset_hf,
            metrics=metrics_to_evaluate,
            llm=langchain_gemini_llm,
            embeddings=custom_embeddings
        )
        logging.info("RAGAS evaluation complete.")
        return result
    except Exception as e:
        logging.error(f"Error during RAGAS evaluation: {e}")
        # Check if it's the OpenAI key error again
        if "OPENAI_API_KEY" in str(e):
            logging.error("RAGAS is still attempting to use OpenAI. This might be due to metrics requiring embeddings not being configured, or a deeper RAGAS default.")
        return None

def main():
    logging.info("Starting RAG evaluation pipeline...")

    evaluation_questions = [
        "What are some quotes about hope and dreams?",
        "Tell me about Oscar Wilde's perspective on art.",
        "Find quotes by female authors on the topic of courage.",
        "What is the meaning of life according to famous philosophers?",
        "Quotes related to perseverance and overcoming adversity."
    ]


    eval_df = prepare_evaluation_data(evaluation_questions)

    if eval_df is not None and not eval_df.empty:
        
        ragas_results = run_ragas_evaluation(eval_df)

        if ragas_results:
            logging.info("\nRAGAS Evaluation Results:")
            results_df = ragas_results.to_pandas()
            logging.info(f"\n{results_df.to_string()}")
            
            results_csv_path = os.path.join("models", "ragas_evaluation_results.csv") # Saving in models for now
            results_df.to_csv(results_csv_path, index=False)
            logging.info(f"RAGAS evaluation results saved to {results_csv_path}")
        else:
            logging.error("RAGAS evaluation failed to produce results.")
    else:
        logging.error("Evaluation data preparation failed. Skipping RAGAS evaluation.")
    
    logging.info("RAG evaluation pipeline finished.")

if __name__ == "__main__":
    main()
